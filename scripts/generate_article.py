#!/usr/bin/env python3
"""
G√©n√©rateur d'articles avec :
- V√©rification des sujets existants sur callrounded.com/blog
- Recherche web via Perplexity
- G√©n√©ration avec OpenAI (style Rounded)
- Validation avant publication
"""

import os
import sys
import json
import requests
import uuid
import random
import string
import re
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from dotenv import load_dotenv
from pathlib import Path
import sys
import os

# Ajouter le chemin parent pour les imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.sanity_utils import html_to_sanity_blocks

load_dotenv()

def convert_text_to_sanity_blocks(text: str) -> list:
    """
    Convertit du texte brut (markdown-like) en format Sanity Block Content
    """
    import random
    import string
    
    def gen_key():
        return ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
    
    blocks = []
    lines = text.split('\n')
    
    current_paragraph = []
    
    for line in lines:
        line = line.strip()
        
        if not line:
            if current_paragraph:
                para_text = ' '.join(current_paragraph)
                blocks.append({
                    "_key": gen_key(),
                    "_type": "block",
                    "style": "normal",
                    "children": [{"_key": gen_key(), "_type": "span", "text": para_text, "marks": []}],
                    "markDefs": []
                })
                current_paragraph = []
            continue
        
        # H2 (##)
        if line.startswith('## '):
            if current_paragraph:
                para_text = ' '.join(current_paragraph)
                blocks.append({
                    "_key": gen_key(),
                    "_type": "block",
                    "style": "normal",
                    "children": [{"_key": gen_key(), "_type": "span", "text": para_text, "marks": []}],
                    "markDefs": []
                })
                current_paragraph = []
            blocks.append({
                "_key": gen_key(),
                "_type": "block",
                "style": "h2",
                "children": [{"_key": gen_key(), "_type": "span", "text": line[3:].strip(), "marks": []}],
                "markDefs": []
            })
            continue
        
        # H3 (###)
        if line.startswith('### '):
            if current_paragraph:
                para_text = ' '.join(current_paragraph)
                blocks.append({
                    "_key": gen_key(),
                    "_type": "block",
                    "style": "normal",
                    "children": [{"_key": gen_key(), "_type": "span", "text": para_text, "marks": []}],
                    "markDefs": []
                })
                current_paragraph = []
            blocks.append({
                "_key": gen_key(),
                "_type": "block",
                "style": "h3",
                "children": [{"_key": gen_key(), "_type": "span", "text": line[4:].strip(), "marks": []}],
                "markDefs": []
            })
            continue
        
        # Liste (‚Ä¢ ou - ou num√©ro)
        if line.startswith('‚Ä¢ ') or line.startswith('- ') or re.match(r'^\d+\.\s', line):
            if current_paragraph:
                para_text = ' '.join(current_paragraph)
                blocks.append({
                    "_key": gen_key(),
                    "_type": "block",
                    "style": "normal",
                    "children": [{"_key": gen_key(), "_type": "span", "text": para_text, "marks": []}],
                    "markDefs": []
                })
                current_paragraph = []
            
            item_text = re.sub(r'^(‚Ä¢|-|\d+\.)\s+', '', line)
            children = parse_marks_for_blocks(item_text)
            
            blocks.append({
                "_key": gen_key(),
                "_type": "block",
                "style": "normal",
                "listItem": "bullet",
                "children": children,
                "markDefs": []
            })
            continue
        
        current_paragraph.append(line)
    
    if current_paragraph:
        para_text = ' '.join(current_paragraph)
        children = parse_marks_for_blocks(para_text)
        blocks.append({
            "_key": gen_key(),
            "_type": "block",
            "style": "normal",
            "children": children,
            "markDefs": []
        })
    
    if not blocks:
        blocks.append({
            "_key": gen_key(),
            "_type": "block",
            "style": "normal",
            "children": [{"_key": gen_key(), "_type": "span", "text": text, "marks": []}],
            "markDefs": []
        })
    
    return blocks


def parse_marks_for_blocks(text: str) -> list:
    """Parse le texte et extrait les marques (gras **texte**)"""
    import random
    import string
    
    def gen_key():
        return ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
    
    children = []
    pattern = r'\*\*(.+?)\*\*'
    parts = re.split(pattern, text)
    
    for i, part in enumerate(parts):
        if not part:
            continue
        if i % 2 == 1:  # Gras
            children.append({
                "_key": gen_key(),
                "_type": "span",
                "text": part,
                "marks": ["strong"]
            })
        else:
            if part.strip():
                children.append({
                    "_key": gen_key(),
                    "_type": "span",
                    "text": part,
                    "marks": []
                })
    
    if not children:
        children.append({
            "_key": gen_key(),
            "_type": "span",
            "text": text,
            "marks": []
        })
    
    return children

# Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")
SANITY_PROJECT_ID = os.getenv("SANITY_PROJECT_ID", "8y6orojx")
SANITY_DATASET = os.getenv("SANITY_DATASET", "development")
SANITY_TOKEN = os.getenv("SANITY_TOKEN")
SANITY_API_URL = f"https://{SANITY_PROJECT_ID}.api.sanity.io/v2025-12-11"

# URLs
ROUNDED_BLOG_URL = "https://callrounded.com/blog"
ROUNDED_DONNA_URL = "https://callrounded.com/cas-usage/secretariat-medical"

# Dossier pour sauvegarder les articles (relatif √† la racine du projet)
BASE_DIR = Path(__file__).parent.parent
ARTICLES_DIR = BASE_DIR / "articles"
ARTICLES_DIR.mkdir(exist_ok=True)

# Initialiser OpenAI
openai_client = None
if OPENAI_API_KEY:
    try:
        from openai import OpenAI
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except:
        pass


def generate_key():
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))


def get_existing_blog_topics() -> List[str]:
    """R√©cup√®re les sujets existants depuis la base de connaissances locale et le site web"""
    print("üîç V√©rification des sujets existants sur le blog Rounded...")
    
    titles = []
    
    # 1. Charger depuis le fichier JSON local (base de connaissances)
    try:
        json_path = BASE_DIR / "data" / "articles_existants.json"
        if json_path.exists():
            with open(json_path, 'r', encoding='utf-8') as f:
                articles = json.load(f)
                titles.extend([article["titre"] for article in articles])
            print(f"‚úÖ {len(titles)} articles charg√©s depuis la base de connaissances locale")
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur chargement base locale: {e}")
    
    # 2. Compl√©ter avec le scraping du site web (optionnel)
    try:
        url = f"{ROUNDED_BLOG_URL}?_cb={int(datetime.now().timestamp() * 1000)}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        html = response.text
        
        # Extraire les titres des articles
        patterns = [
            r'<h[23][^>]*>([^<]+)</h[23]>',
            r'Lire l\'article[^>]*>([^<]+)</a>',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            titles.extend([m.strip() for m in matches if len(m.strip()) > 10])
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Scraping web √©chou√© (non bloquant): {e}")
    
    # Nettoyer et d√©dupliquer
    titles = list(set([t for t in titles if t and len(t) > 10]))
    
    print(f"‚úÖ {len(titles)} articles existants trouv√©s au total")
    if titles:
        print("   Exemples:", titles[:3])
    
    return titles


def check_topic_exists(topic: str, existing_topics: List[str]) -> bool:
    """V√©rifie si un sujet est trop similaire aux articles existants"""
    if not existing_topics:
        return False
    
    topic_lower = topic.lower()
    
    # Mots-cl√©s du sujet (enlever les mots communs)
    stop_words = {'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'et', 'ou', '√†', 'en', 'pour', 'avec', 'sur', 'dans', 'par', 'comment', 'pourquoi', 'quand', 'que', 'qui', 'quoi'}
    topic_words = set([w for w in topic_lower.split() if w not in stop_words and len(w) > 2])
    
    similar_articles = []
    
    for existing in existing_topics:
        existing_lower = existing.lower()
        existing_words = set([w for w in existing_lower.split() if w not in stop_words and len(w) > 2])
        
        # V√©rifier le chevauchement des mots-cl√©s importants
        if len(topic_words) > 0:
            overlap = len(topic_words & existing_words)
            # Similarit√© bas√©e sur les mots significatifs
            similarity = overlap / max(len(topic_words), len(existing_words))
            
            # Si plus de 35% de similarit√©, c'est probablement trop similaire
            if similarity > 0.35:
                similar_articles.append((existing, similarity))
    
    if similar_articles:
        # Trier par similarit√© d√©croissante
        similar_articles.sort(key=lambda x: x[1], reverse=True)
        print(f"\n‚ö†Ô∏è  {len(similar_articles)} article(s) similaire(s) trouv√©(s):")
        for article, sim in similar_articles[:3]:
            print(f"   - '{article}' (similarit√©: {sim:.0%})")
        return True
    
    return False


def search_web(query: str) -> str:
    """Recherche web via Perplexity - retourne le contenu"""
    result = search_web_with_sources(query)
    return result.get("content", "") if isinstance(result, dict) else result

def search_web_with_sources(query: str) -> dict:
    """Recherche web via Perplexity avec extraction des sources"""
    if not PERPLEXITY_API_KEY:
        return {"content": "", "sources": []}
    
    print("üîç Recherche web via Perplexity...")
    
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "sonar-pro",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant specialized in medical practices, healthcare technology, and voice AI assistants."},
            {"role": "user", "content": query}
        ]
    }
    
    try:
        response = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        data = response.json()
        
        # Extraire le contenu
        content = data["choices"][0]["message"]["content"]
        
        # Extraire les citations/sources
        sources = []
        
        # Perplexity retourne les citations dans diff√©rents formats selon le mod√®le
        # Format 1: Dans la r√©ponse principale
        if "citations" in data:
            sources = data["citations"]
        # Format 2: Dans le message
        elif "choices" in data and len(data["choices"]) > 0:
            message = data["choices"][0]["message"]
            if "citations" in message:
                sources = message["citations"]
        
        # Format 3: Dans les m√©tadonn√©es de la r√©ponse
        if not sources and "choices" in data:
            choice = data["choices"][0]
            if "citations" in choice:
                sources = choice["citations"]
        
        # Si pas de citations structur√©es, essayer d'extraire les URLs du contenu
        if not sources:
            import re
            from urllib.parse import urlparse
            
            # Chercher les URLs dans le contenu (format [1], [2], etc. ou URLs directes)
            url_pattern = r'https?://[^\s\)\]\>]+'
            urls = re.findall(url_pattern, content)
            if urls:
                # Nettoyer les URLs (enlever les caract√®res de fin)
                cleaned_urls = []
                for url in urls:
                    # Enlever les caract√®res de ponctuation √† la fin
                    url = url.rstrip('.,;:!?)')
                    if url not in cleaned_urls:
                        cleaned_urls.append(url)
                
                # Cr√©er des objets source avec domaine extrait
                sources = []
                for url in cleaned_urls:
                    try:
                        parsed = urlparse(url)
                        domain = parsed.netloc
                        sources.append({
                            "url": url,
                            "domain": domain,
                            "name": domain.replace("www.", "")
                        })
                    except:
                        sources.append({"url": url})
        
        # Normaliser les sources (s'assurer qu'elles sont toutes des dicts avec m√©tadonn√©es)
        normalized_sources = []
        for source in sources:
            if isinstance(source, dict):
                # Enrichir avec des m√©tadonn√©es si manquantes
                if "url" in source and "domain" not in source:
                    try:
                        from urllib.parse import urlparse
                        parsed = urlparse(source["url"])
                        source["domain"] = parsed.netloc
                        if "name" not in source:
                            source["name"] = parsed.netloc.replace("www.", "")
                    except:
                        pass
                
                # S'assurer qu'il y a au moins un titre/name
                if "title" not in source and "name" not in source:
                    if "domain" in source:
                        source["name"] = source["domain"].replace("www.", "")
                    elif "url" in source:
                        try:
                            from urllib.parse import urlparse
                            parsed = urlparse(source["url"])
                            source["name"] = parsed.netloc.replace("www.", "")
                        except:
                            source["name"] = "Source"
                
                normalized_sources.append(source)
            elif isinstance(source, str):
                # Si c'est juste une URL string, cr√©er un dict
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(source)
                    normalized_sources.append({
                        "url": source,
                        "domain": parsed.netloc,
                        "name": parsed.netloc.replace("www.", "")
                    })
                except:
                    normalized_sources.append({"url": source, "name": "Source"})
        
        print("‚úÖ Recherche termin√©e")
        return {
            "content": content,
            "sources": normalized_sources
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur recherche Perplexity: {e}")
        return {"content": "", "sources": []}


def load_existing_articles() -> List[Dict[str, Any]]:
    """Charge tous les articles existants depuis data/articles_existants.json"""
    json_path = BASE_DIR / "data" / "articles_existants.json"
    try:
        if not json_path.exists():
            return []
        with open(json_path, "r", encoding="utf-8") as f:
            articles = json.load(f)
        return articles if isinstance(articles, list) else []
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur chargement articles existants: {e}")
        return []


def generate_topic_variants(
    topic: str,
    existing_articles: List[Dict[str, Any]],
    target_keywords: Optional[List[str]] = None,
) -> List[Dict[str, Any]]:
    """
    G√©n√®re 3 variantes de sujets √† partir d'un sujet brut :
    - titre en une phrase
    - angle √©ditorial
    - mini-plan (3‚Äì5 points)
    """
    if not openai_client:
        raise ValueError("OPENAI_API_KEY non configur√©e dans .env")

    print("\nüß† G√©n√©ration de 3 variantes de sujets (titre + mini-plan)...")

    existing_titles = [art.get("titre", "") for art in existing_articles if art.get("titre")]
    existing_titles_snippet = "\n".join(f"- {t}" for t in existing_titles[:20]) if existing_titles else ""

    keywords_snippet = ", ".join(target_keywords or []) if target_keywords else ""

    system_prompt = """Tu es un content strategist SEO pour une startup SaaS sp√©cialis√©e dans les agents vocaux IA (comme Donna chez Rounded).

Ton r√¥le : proposer des id√©es d'articles de blog B2B tr√®s cibl√©es, diff√©renci√©es de l'existant, adapt√©es aux d√©cideurs (direction, responsables op√©rations, m√©decins, etc.). 
Tu ne cites JAMAIS de concurrents ni de marques tierces (airagent.ai, etc.). Tu t'en inspires uniquement pour le niveau de sophistication, jamais dans le nom.

Contraintes :
- Ne JAMAIS r√©p√©ter exactement un titre existant.
- √âviter les doublons de sujets d√©j√† trait√©s.
- Int√©grer naturellement les mots-cl√©s fournis quand c'est pertinent (sans keyword stuffing).
- Chaque variante doit avoir un angle bien distinct (probl√©matique, comparatif, cas d'usage, ROI, etc.).

Format de sortie : un objet JSON avec un tableau 'variants' de 3 √©l√©ments. 
Chaque √©l√©ment doit avoir :
- title: string (titre en une phrase, max 90 caract√®res)
- angle: string (1‚Äì2 phrases qui expliquent l'angle √©ditorial)
- outline: array de 3 √† 5 puces (mini-plan de l'article)
"""

    # D√©tecter le secteur cible depuis le sujet
    sector_hint = ""
    topic_lower = topic.lower()
    if any(word in topic_lower for word in ["syndic", "copropri√©t√©", "gestionnaire immobilier", "immobilier"]):
        sector_hint = "syndics immobiliers / gestionnaires de copropri√©t√©s"
    elif any(word in topic_lower for word in ["m√©dical", "m√©decin", "cabinet m√©dical", "secr√©taire m√©dicale"]):
        sector_hint = "cabinets m√©dicaux / secr√©tariat m√©dical"
    elif any(word in topic_lower for word in ["agence immobili√®re", "immobilier"]):
        sector_hint = "agences immobili√®res"
    else:
        sector_hint = "entreprises / professionnels cherchant √† automatiser leur accueil t√©l√©phonique"
    
    user_prompt = {
        "topic_seed": topic,
        "existing_titles": existing_titles_snippet,
        "target_keywords": keywords_snippet,
        "instructions": f"Propose 3 id√©es d'articles diff√©rentes mais coh√©rentes avec le sujet de d√©part '{topic}', en √©vitant les doublons avec les titres existants. L'article doit √™tre adapt√© au secteur : {sector_hint}. Concentre-toi STRICTEMENT sur le secteur mentionn√© dans le sujet de d√©part, ne d√©vie pas vers d'autres secteurs."
    }

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(user_prompt, ensure_ascii=False)},
            ],
            response_format={"type": "json_object"},
            temperature=0.8,
            max_tokens=1200,
        )
        
        # Tracker les tokens
        if hasattr(response, 'usage') and response.usage:
            try:
                from utils.token_tracker import track_openai_usage
                track_openai_usage(
                    operation="generate_variants",
                    model="gpt-4o-mini",
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    topic=topic
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur tracking tokens: {e}")
        
        data = json.loads(response.choices[0].message.content)
        variants = data.get("variants") or data.get("ideas") or []
        # Normaliser un minimum
        cleaned: List[Dict[str, Any]] = []
        for v in variants[:3]:
            title = v.get("title") or v.get("titre") or ""
            angle = v.get("angle") or ""
            outline = v.get("outline") or v.get("plan") or []
            if not isinstance(outline, list):
                outline = [str(outline)]
            if title:
                cleaned.append(
                    {
                        "title": title.strip(),
                        "angle": angle.strip(),
                        "outline": [str(p).strip() for p in outline if str(p).strip()],
                    }
                )
        # S'assurer d'avoir 3 √©l√©ments (au pire dupliqu√©s)
        while len(cleaned) < 3 and cleaned:
            cleaned.append(cleaned[len(cleaned) - 1])

        print("‚úÖ 3 variantes de sujets g√©n√©r√©es")
        return cleaned[:3]
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration variantes de sujets: {e}")
        raise


def generate_article(
    variant: Dict[str, Any],
    web_results: str = "",
    target_keywords: Optional[List[str]] = None,
) -> str:
    """G√©n√®re l'article complet √† partir d'une variante (titre + angle + mini-plan)."""
    if not openai_client:
        raise ValueError("OPENAI_API_KEY non configur√©e dans .env")

    title = variant.get("title", "").strip()
    angle = variant.get("angle", "").strip()
    outline = variant.get("outline", []) or []

    print(f"üìù G√©n√©ration de l'article complet pour la variante choisie : {title}")

    current_date = datetime.now()
    year = current_date.year
    readable_date = current_date.strftime("%d/%m/%Y")

    keywords_snippet = ", ".join(target_keywords or []) if target_keywords else ""

    # Style bas√© sur les exemples fournis
    system_prompt = f"""Tu es un r√©dacteur professionnel sp√©cialis√© dans les agents vocaux IA, le secr√©tariat m√©dical, et les technologies de t√©l√©phonie automatis√©e pour les cabinets m√©dicaux.

DATE ACTUELLE: {readable_date} ({year})

STYLE ET TON:
- Professionnel mais accessible, humain
- Structure claire avec des points num√©rot√©s (1., 2., 3., etc.) pour les sections principales uniquement
- √âvite les num√©ros dans les sous-titres et sous-sections
- Chaque point commence par un titre court et percutant
- Utilise des exemples concrets et des situations r√©elles
- Ton rassurant et pratique pour les professionnels de sant√©
- Mentionne Donna (l'assistante vocale m√©dicale de Rounded) de mani√®re naturelle quand pertinent
- Ne force jamais la promotion, sois subtil et informatif
- INTERDICTION ABSOLUE : Ne jamais mentionner de concurrents ou d'autres entreprises (CareCall, Plateya, Talan, airagent.ai, etc.). Utilise des formulations g√©n√©riques comme "des √©tudes r√©centes", "certaines solutions IA", etc.

STRUCTURE TYPE:
1. Introduction : contexte du probl√®me (2-3 paragraphes)
2. Points principaux num√©rot√©s (3-6 points avec titres courts)
3. Conclusion : r√©sum√© + mention de Donna si pertinent + lien vers https://callrounded.com/cas-usage/secretariat-medical

MENTIONS DE DONNA:
- Mentionne Donna UNIQUEMENT si l'article traite sp√©cifiquement de l'IA vocale dans le secr√©tariat m√©dical
- Si l'article parle d'un autre secteur (immobilier, copropri√©t√©, etc.) ou d'un sujet non li√© √† l'IA vocale m√©dicale, NE PAS mentionner Donna
- Si pertinent (IA vocale + secr√©tariat m√©dical), dans la conclusion, propose le lien : D√©couvrir Donna : https://callrounded.com/cas-usage/secretariat-medical
- Ne pas sur-promouvoir, rester informatif
- Si le sujet n'est PAS en rapport avec l'IA vocale pour secr√©tariat m√©dical, NE PAS ajouter de lien vers Donna

FORMAT:
- Minimum 1200 mots
- Utilise Markdown avec # pour les titres principaux
- ## pour les sous-titres de sections
- **gras** pour les points importants
- Listes √† puces quand pertinent
- Paragraphes courts et a√©r√©s

CONTEXTE:
Tu √©cris pour des professionnels de sant√© (secr√©taires m√©dicales, m√©decins, responsables de cabinets) qui cherchent des solutions pratiques pour am√©liorer leur organisation.

SEO:
- Int√®gre naturellement les mots-cl√©s suivants quand c'est pertinent (sans sur-optimisation) : {keywords_snippet}."""

    plan_str = "\n".join(f"- {p}" for p in outline) if outline else ""

    user_prompt = f"""Titre de l'article: {title}

Angle √©ditorial √† adopter:
{angle}

Plan (structure principale √† respecter, tu peux d√©tailler mais pas changer l'intention des points) :
{plan_str}

Donn√©es de recherche web r√©centes (utiliser comme source d'informations, sans copier/coller brut) :
{web_results[:3000] if web_results else "Aucune donn√©e sp√©cifique fournie. Utilise tes connaissances actuelles."}

IMPORTANT:
- √âcris un article complet de minimum 1200 mots
- Utilise le plan fourni comme colonne vert√©brale de l'article
- Structure avec des points num√©rot√©s clairs (1., 2., 3.) pour les sections principales uniquement
- √âvite les num√©ros dans les sous-titres
- Style professionnel mais accessible
- Mentionne Donna UNIQUEMENT si l'article traite sp√©cifiquement de l'IA vocale dans le secr√©tariat m√©dical
- Si l'article parle d'un autre secteur (immobilier, copropri√©t√©, etc.) ou d'un sujet non li√© √† l'IA vocale m√©dicale, NE PAS mentionner Donna du tout
- INTERDICTION ABSOLUE : Ne jamais citer de concurrents ou d'autres entreprises. Utilise des formulations g√©n√©riques.
- Termine par une conclusion qui r√©sume les points cl√©s
- Si et SEULEMENT SI pertinent (IA vocale + secr√©tariat m√©dical), termine par un appel √† d√©couvrir Donna avec le lien https://callrounded.com/cas-usage/secretariat-medical
- Si le sujet n'est PAS en rapport avec l'IA vocale pour secr√©tariat m√©dical, NE PAS ajouter de lien vers Donna dans la conclusion

G√©n√®re l'article maintenant."""

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.8,
            max_tokens=4000,
        )
        
        # Tracker les tokens
        if hasattr(response, 'usage') and response.usage:
            try:
                from utils.token_tracker import track_openai_usage
                variant_title = variant.get("title", "")
                track_openai_usage(
                    operation="generate_article",
                    model="gpt-4o-mini",
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    article_title=variant_title
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur tracking tokens: {e}")
        
        article = response.choices[0].message.content
        print("‚úÖ Article complet g√©n√©r√©")
        return article
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration: {e}")
        raise


def apply_style_refinement(article: str) -> str:
    """
    Applique un raffinement de style √† l'article g√©n√©r√©.

    Objectif :
    - Donner du "grain" au texte
    - Rendre la lecture plus rythm√©e et concr√®te
    """
    if not openai_client:
        # Si pas de client OpenAI, on renvoie l'article tel quel
        return article

    style_prompt = """
Tu es un r√©dacteur senior B2B fran√ßais, ton de marque Rounded : expert, direct, un peu mordant mais jamais vulgaire.

Tu re√ßois ci-dessous un article d√©j√† structur√© (H2/H3, paragraphes, listes). 
Ta mission : R√â√âCRIRE l‚Äôarticle COMPLET en respectant ces r√®gles :

1) Structure des phrases
- Varie la ponctuation : points, virgules, deux-points, points-virgules, parenth√®ses (avec mod√©ration).
- √âvite les phrases trop longues : id√©alement 12‚Äì22 mots, rarement plus de 30.
- Commence autant que possible chaque phrase par l‚Äôid√©e cl√© (th√®se), puis 
  seulement ensuite l‚Äôexplication, puis un exemple, un chiffre ou une mini-citation.

2) Ton & voix
- Garde un ton professionnel, clair, orient√© business.
- Autoris√© : un peu de sarcasme / ironie l√©g√®re pour pointer les absurdit√©s du r√©el 
  (par ex. ‚Äú√©videmment, personne n‚Äôa jamais eu un appel perdu un lundi matin‚Ä¶‚Äù).
- 1 √† 3 blagues maxi sur tout l‚Äôarticle, subtiles, jamais lourdes, jamais sur les patients.

3) Progression & transitions
- Ajoute UNE courte phrase de transition entre chaque H2 pour faire le lien logique
  (ex : ‚ÄúAvant de parler co√ªts, regardons d‚Äôabord ce qui coince au quotidien.‚Äù).
- Les transitions doivent √™tre naturelles et orienter la suite de la lecture.

4) Anecdotes & concret
- Quand c‚Äôest pertinent, ajoute de petites anecdotes r√©alistes (2‚Äì3 phrases) :
  - situations de secr√©tariat m√©dical
  - appels rat√©s, d√©bordement, patients frustr√©s, m√©decins d√©bord√©s, etc.
- Ces anecdotes doivent rester cr√©dibles, pas romanc√©es.

5) Sources & chiffres cl√©s
- Si l‚Äôarticle contient d√©j√† des chiffres, √©tudes, pourcentages : 
  - mets-les davantage en valeur (formulations percutantes, ‚ÄúEn clair‚Ä¶‚Äù, ‚ÄúConcr√®tement‚Ä¶‚Äù).
- Si tu vois passer des noms d‚Äô√©tudes, organismes, sources : 
  - reformule en une phrase qui donne du contexte (‚ÄúUne √©tude de [organisme] montre que‚Ä¶‚Äù).
- NE PAS inventer de chiffres qui ne sont pas d√©j√† pr√©sents dans le texte.

6) Mise en forme
- Garde STRICTEMENT la structure H2/H3 / listes.
- Ne change pas le sens business g√©n√©ral ni les messages cl√©s.
- Ne rajoute PAS de nouveaux liens vers Donna ou Rounded au-del√† de ce qui est pr√©vu
  dans la version originale.

Retourne UNIQUEMENT l‚Äôarticle r√©√©crit, au format Markdown, sans commentaire autour.
"""

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": style_prompt},
                {
                    "role": "user",
                    "content": (
                        "Voici l'article √† r√©√©crire en appliquant STRICTEMENT les r√®gles ci-dessus :\n\n"
                        f"{article}"
                    ),
                },
            ],
            temperature=0.8,
            max_tokens=4000,
        )

        styled_article = response.choices[0].message.content

        # Tracking tokens pour la passe de style
        if hasattr(response, "usage") and response.usage:
            try:
                from utils.token_tracker import track_openai_usage

                track_openai_usage(
                    operation="style_refinement",
                    model="gpt-4o-mini",
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens,
                    },
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur tracking tokens (style_refinement): {e}")

        return styled_article or article

    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur apply_style_refinement: {e}")
        # En cas de probl√®me, on ne bloque pas : on garde l'article brut
        return article


def score_article_quality(article: str, topic: str, target_keywords: Optional[List[str]] = None, article_title: Optional[str] = None) -> Dict[str, Any]:
    """
    √âvalue l'article et retourne un rapport de scoring (√©ditorial + SEO) au format structur√©.

    Retour :
    {
        "global_score": int | None,
        "content_score": int | None,
        "readability_score": int | None,
        "seo_score": int | None,
        "conversion_score": int | None,
        "credibility_score": int | None,
        "markdown": str  # rapport complet en Markdown (style exemple utilisateur)
    }
    """
    if not openai_client:
        return {
            "global_score": None,
            "content_score": None,
            "readability_score": None,
            "seo_score": None,
            "conversion_score": None,
            "credibility_score": None,
            "markdown": "",
        }

    keywords_str = ", ".join(target_keywords or []) if target_keywords else ""

    scoring_system_prompt = """
Tu es un expert en √©valuation de contenu √©ditorial et SEO pour des articles de blog B2B.

Ta mission : analyser CHAQUE article de mani√®re INDIVIDUELLE et donner un scoring UNIQUE 
bas√© sur les caract√©ristiques R√âELLES de cet article sp√©cifique.

IMPORTANT : 
- Chaque article doit avoir un scoring DIFF√âRENT selon son contenu r√©el
- Analyse en profondeur : longueur, structure, qualit√© des arguments, pr√©sence de chiffres, 
  anecdotes, transitions, CTA, FAQ, etc.
- Sois STRICT et VARI√â dans tes scores : ne donne pas toujours les m√™mes notes
- Un article avec beaucoup de chiffres et d'exemples concrets aura un meilleur score contenu
- Un article avec une FAQ et plusieurs CTA aura un meilleur score conversion
- Un article avec des phrases courtes et bien structur√©es aura un meilleur score lisibilit√©
- Adapte tes scores √† la R√âALIT√â de l'article, pas √† un standard g√©n√©rique
"""

    article_title_context = f"\n- Titre de l'article : {article_title}" if article_title else ""
    
    scoring_user_prompt = f"""
CONTEXTE :
- Sujet de l'article : {topic}
- Mots-cl√©s cibl√©s : {keywords_str if keywords_str else "non pr√©cis√©s"}{article_title_context}

ARTICLE √Ä √âVALUER (analyse-le en profondeur, caract√®re par caract√®re) :
---
{article}
---

INSTRUCTIONS D'√âVALUATION :

1. ANALYSE APPROFONDIE :
   - Compte r√©ellement les mots, phrases, paragraphes
   - Identifie les chiffres, statistiques, exemples concrets
   - Rep√®re les CTA, FAQ, transitions, anecdotes
   - √âvalue la structure H2/H3, listes, formatage
   - Mesure la longueur r√©elle des phrases
   - V√©rifie la pr√©sence et r√©p√©tition des mots-cl√©s

2. SCORING PERSONNALIS√â (sur 100 pour le global, sur 20 pour chaque dimension) :
   - Score global : bas√© sur la moyenne pond√©r√©e des dimensions
   - Qualit√© du contenu (0-20) : arguments solides, exemples concrets, chiffres, profondeur
   - Lisibilit√© & clart√© (0-20) : phrases courtes, structure claire, vocabulaire adapt√©
   - SEO (0-30) : mots-cl√©s pr√©sents, r√©p√©tition strat√©gique, structure H2/H3, m√©ta
   - Conversion & marketing (0-20) : CTA pr√©sents, FAQ, appels √† l'action, b√©n√©fices clairs
   - Cr√©dibilit√© secteur sant√© (0-10) : ton respectueux, pas de promesses irr√©alistes

3. VARIATION DES SCORES :
   - Si l'article est court (< 800 mots) : p√©nalise le score contenu
   - Si l'article n'a pas de FAQ : p√©nalise le score conversion
   - Si les phrases sont tr√®s longues (> 30 mots) : p√©nalise la lisibilit√©
   - Si les mots-cl√©s sont absents : p√©nalise fortement le SEO
   - Si l'article a beaucoup de chiffres et d'exemples : bon score contenu
   - ADAPTE les scores √† la R√âALIT√â de cet article sp√©cifique

4. RAPPORT D√âTAILL√â :
   - Commence par le score global avec un commentaire personnalis√©
   - D√©taille chaque dimension avec des exemples CONCRETS tir√©s de l'article
   - Liste les points forts R√âELS de cet article
   - Liste les points faibles R√âELS de cet article
   - Propose 5 actions PRIORITAIRES pour am√©liorer CET article sp√©cifique
   - Inclus un tableau "Score par type d'outil simul√©" (SEO Review Tools, Hemingway, etc.)

FORMAT DU RAPPORT :
- Style professionnel avec emojis (üìä, ‚úçÔ∏è, üìñ, üîç, üéØ, üè•)
- Titres clairs : "Score global", "D√©tail du scoring", "5 actions pour passer √† 90+"
- Tableaux pour les scores par outil
- Listes √† puces pour les recommandations
- En fran√ßais, ton expert mais accessible

IMPORTANT - FORMAT JSON STRICT :
{{
  "global_score": <score entre 50 et 95, VARI√â selon l'article>,
  "content_score": <score entre 10 et 20>,
  "readability_score": <score entre 10 et 20>,
  "seo_score": <score entre 15 et 30>,
  "conversion_score": <score entre 8 et 20>,
  "credibility_score": <score entre 8 et 10>,
  "markdown_report": "<rapport complet en Markdown, tr√®s d√©taill√© et personnalis√© pour CET article>"
}}

Ne renvoie QUE le JSON, sans texte autour.
"""

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": scoring_system_prompt},
                {"role": "user", "content": scoring_user_prompt},
            ],
            response_format={"type": "json_object"},
            temperature=0.4,
            max_tokens=2000,
        )

        data = json.loads(response.choices[0].message.content)

        result = {
            "global_score": data.get("global_score"),
            "content_score": data.get("content_score"),
            "readability_score": data.get("readability_score"),
            "seo_score": data.get("seo_score"),
            "conversion_score": data.get("conversion_score"),
            "credibility_score": data.get("credibility_score"),
            "markdown": data.get("markdown_report", ""),
        }

        # Tracking tokens
        if hasattr(response, "usage") and response.usage:
            try:
                from utils.token_tracker import track_openai_usage

                track_openai_usage(
                    operation="score_article",
                    model="gpt-4o-mini",
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens,
                    },
                    topic=topic,
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur tracking tokens (score_article): {e}")

        return result

    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur score_article_quality: {e}")
        return {
            "global_score": None,
            "content_score": None,
            "readability_score": None,
            "seo_score": None,
            "conversion_score": None,
            "credibility_score": None,
            "markdown": "",
        }


def regenerate_article_with_scoring(
    article: str,
    scoring_markdown: str,
    topic: str,
    target_keywords: Optional[List[str]] = None,
    max_iterations: int = 3,
) -> str:
    """
    R√©g√©n√®re l'article en s'appuyant sur le scoring et les recommandations.

    - Objectif : passer d'un bon article √† un article optimis√© (90+ / 100).
    - Ne doit PAS changer le message de fond, mais am√©liorer :
      structure, SEO, conversion, clart√©, impact.
    """
    if not openai_client:
        return article

    keywords_str = ", ".join(target_keywords or []) if target_keywords else ""

    system_prompt = """
Tu es un expert en copywriting B2B et SEO pour le secteur m√©dical, travaillant pour Rounded.

Ta mission CRITIQUE : am√©liorer l'article en appliquant TOUTES les recommandations du scoring.

R√àGLES STRICTES :
1. L'article am√©lior√© DOIT avoir un meilleur score que l'article initial
2. Applique TOUTES les recommandations du rapport de scoring :
   - Si le rapport dit "ajouter une FAQ" ‚Üí AJOUTE une FAQ compl√®te MAIS PLACE-LA √Ä LA FIN, juste avant la conclusion
   - Si le rapport dit "renforcer les CTA" ‚Üí AJOUTE des CTA clairs et visibles (mais pas en plein milieu du contenu)
   - Si le rapport dit "raccourcir les phrases" ‚Üí RACCOURCIS les phrases longues
   - Si le rapport dit "r√©p√©ter le mot-cl√©" ‚Üí R√âP√àTE le mot-cl√© strat√©giquement
   - Si le rapport dit "am√©liorer la conversion" ‚Üí AJOUTE des appels √† l'action en fin de sections pertinentes
3. STRUCTURE OBLIGATOIRE :
   - Introduction
   - Sections H2 principales (contenu de l'article)
   - FAQ (si recommand√©e) ‚Üí TOUJOURS √† la fin, juste avant la conclusion
   - Conclusion
4. Garde le m√™me angle, la m√™me cible et les m√™mes messages business
5. Garde la structure g√©n√©rale (H2/H3, listes) mais am√©liore-la selon les recommandations
6. N'invente PAS de nouveaux chiffres pr√©cis si aucun chiffre n'√©tait pr√©sent
7. Ne rajoute PAS de nouveaux liens externes non mentionn√©s dans l'article initial
8. NE PLACE JAMAIS la FAQ en plein milieu de l'article - elle doit √™tre √† la fin, juste avant la conclusion

OBJECTIF : L'article final DOIT √™tre meilleur que l'initial sur TOUS les crit√®res mentionn√©s dans le scoring, avec une structure propre et professionnelle.

Format attendu :
- Retourne UNIQUEMENT l'article r√©√©crit et am√©lior√©, en Markdown propre.
"""

    user_prompt = f"""
Contexte :
- Sujet : {topic}
- Mots-cl√©s cibles indicatifs : {keywords_str}

ARTICLE INITIAL :
---
{article}
---

RAPPORT DE SCORING & RECOMMANDATIONS :
---
{scoring_markdown}
---

Maintenant, r√©√©cris l'article COMPLET en appliquant les recommandations.

STRUCTURE FINALE OBLIGATOIRE :
1. Introduction
2. Sections H2 principales (contenu de l'article)
3. FAQ (si recommand√©e dans le scoring) ‚Üí PLACER ICI, juste avant la conclusion, JAMAIS en plein milieu
4. Conclusion

IMPORTANT : Si tu ajoutes une FAQ, elle DOIT √™tre plac√©e √† la fin de l'article, juste avant la conclusion. 
NE PLACE JAMAIS la FAQ en plein milieu du contenu - cela casse la structure et n'est pas professionnel.

Retourne UNIQUEMENT l'article r√©√©crit en Markdown, sans commentaire autour.
"""

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7,
            max_tokens=4000,
        )

        improved_article = response.choices[0].message.content

        # Tracking tokens
        if hasattr(response, "usage") and response.usage:
            try:
                from utils.token_tracker import track_openai_usage

                track_openai_usage(
                    operation="regenerate_with_scoring",
                    model="gpt-4o-mini",
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens,
                    },
                    topic=topic,
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur tracking tokens (regenerate_with_scoring): {e}")

        return improved_article or article

    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur regenerate_article_with_scoring: {e}")
    return article


def load_target_keywords() -> List[str]:
    """Charge les mots-cl√©s cibles depuis data/keywords.json (si pr√©sent)"""
    json_path = Path("data/keywords.json")
    try:
        if not json_path.exists():
            return []
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # Accepte soit une simple liste, soit un dict avec une cl√© "default"
        if isinstance(data, list):
            keywords = data
        elif isinstance(data, dict):
            # On prend "default" si pr√©sent, sinon le premier ensemble trouv√©
            if "default" in data and isinstance(data["default"], list):
                keywords = data["default"]
            else:
                first = next((v for v in data.values() if isinstance(v, list)), [])
                keywords = first
        else:
            return []
        # Nettoyage simple
        return [str(k).strip() for k in keywords if str(k).strip()]
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur chargement des mots-cl√©s (data/keywords.json): {e}")
        return []


def select_target_keywords(topic: str, all_keywords: List[str], min_k: int = 2, max_k: int = 4) -> List[str]:
    """
    S√©lectionne entre 2 et 4 mots-cl√©s pertinents en fonction du sujet.

    Logique simple :
    - On score chaque mot-cl√© selon son recouvrement avec les mots du sujet
    - On prend les meilleurs scores en priorit√©
    - On garantit au moins min_k mots-cl√©s (en compl√©tant si besoin)
    """
    if not topic or not all_keywords:
        return all_keywords[:max_k]

    topic_lower = topic.lower()
    # D√©couper le sujet en mots significatifs
    topic_words = re.findall(r"[a-z√†√¢√§√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ß0-9]+", topic_lower)
    topic_words_set = set(topic_words)

    scored: List[Tuple[int, str]] = []
    for kw in all_keywords:
        kw_lower = kw.lower()
        kw_words = re.findall(r"[a-z√†√¢√§√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ß0-9]+", kw_lower)
        kw_words_set = set(kw_words)

        # Score = nombre de mots en commun + bonus si le mot-cl√© est un substring du sujet
        common_words = topic_words_set.intersection(kw_words_set)
        score = len(common_words)
        if kw_lower in topic_lower or any(w in topic_lower for w in kw_words_set):
            score += 1

        scored.append((score, kw))

    # Trier par score d√©croissant
    scored.sort(key=lambda x: x[0], reverse=True)

    # Garder uniquement les mots-cl√©s avec un score > 0 en priorit√©
    positive = [kw for score, kw in scored if score > 0]

    selected: List[str] = []
    for kw in positive:
        if kw not in selected:
            selected.append(kw)
        if len(selected) >= max_k:
            break

    # Si on n'a pas assez de mots-cl√©s, compl√©ter avec les autres (sans doublon)
    if len(selected) < min_k:
        for _score, kw in scored:
            if kw not in selected:
                selected.append(kw)
            if len(selected) >= min_k:
                break

    return selected[:max_k]


def optimize_seo(article: str, target_keywords: Optional[List[str]] = None) -> Dict[str, Any]:
    """Optimise SEO et retourne les m√©tadonn√©es compl√®tes"""
    if not openai_client:
        # Fallback simple
        slug = article[:50].lower().replace(' ', '-').replace("'", '').replace(",", '').replace("?", '').replace(".", '')
        slug = re.sub(r'[^a-z0-9-]', '', slug)
        title = article.split('\n')[0].replace('#', '').strip()[:60]
        summary = article[:155]
        return {
            "title": title,
            "summary": summary,
            "blog_post": article,
            "slug": slug,
            "readTime": "5 min",
            "tag": "actualites-tendances",
            "keywords": target_keywords or [],
            "metaTitle": title[:60],
            "metaDescription": summary[:160],
            "ogTitle": title,
            "ogDescription": summary[:160],
            "canonicalUrl": f"https://callrounded.com/blog/{slug}",
            "translationGroup": slug
        }
    
    print("üîç Optimisation SEO...")
    
    system_prompt = """You are an expert SEO copywriter. Analyze the article and return JSON with ALL these fields:
- title: SEO-optimized title (max 65 chars, include main keyword)
- summary: Meta description for excerpt (155-160 chars, compelling)
- blog_post: Full HTML content with proper tags (<h2>, <h3>, <p>, <ul>, <li>, <strong>)
- slug: URL-friendly slug (lowercase, hyphens, no special chars)
- readTime: Reading time estimation (e.g., "5 min", "8 min")
- tag: Category from: actualites-tendances, guides-pratiques
- keywords: Array of 5-8 relevant keywords
- focusKeyword: Main keyword (1-2 words)
- metaTitle: SEO title for meta tag (50-60 chars)
- metaDescription: SEO meta description (155-160 chars)
- ogTitle: Open Graph title (can be same as title or slightly different)
- ogDescription: Open Graph description (155-160 chars)
- canonicalUrl: Full canonical URL (https://callrounded.com/blog/{slug})
- translationGroup: Translation group ID (same as slug)

Convert Markdown to HTML:
- # Title ‚Üí <h2>Title</h2>
- ## Subtitle ‚Üí <h3>Subtitle</h3>
- **bold** ‚Üí <strong>bold</strong>
- * item ‚Üí <ul><li>item</li></ul>
- Regular text ‚Üí <p>text</p>"""
    # Si des mots-cl√©s cibles existent, on les ajoute explicitement au prompt SEO
    if target_keywords:
        joined = ", ".join(target_keywords)
        system_prompt += f"\n\nIMPORTANT:\n- You MUST prioritize and naturally include the following SEO keywords when relevant: {joined}.\n- Return them in the 'keywords' array field (add more if useful, but always keep these)."
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": article}
            ],
            response_format={"type": "json_object"},
            temperature=0.7
        )
        
        # Tracker les tokens
        if hasattr(response, 'usage') and response.usage:
            try:
                from utils.token_tracker import track_openai_usage
                track_openai_usage(
                    operation="optimize_seo",
                    model="gpt-4o-mini",
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    article_title=result.get("title", "") if 'result' in locals() else None
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur tracking tokens: {e}")
        
        result = json.loads(response.choices[0].message.content)
        
        # Nettoyer les titres pour enlever les caract√®res probl√©matiques
        title = result.get("title", "").strip()
        title = title.replace('\n', ' ').replace('\r', ' ')
        title = re.sub(r'\s+', ' ', title)
        result["title"] = title
        
        # S'assurer que tous les champs sont pr√©sents
        slug = result.get("slug", article[:50].lower().replace(' ', '-'))
        
        meta_title = result.get("metaTitle", title)[:60].strip()
        meta_title = meta_title.replace('\n', ' ').replace('\r', ' ')
        meta_title = re.sub(r'\s+', ' ', meta_title)
        result["metaTitle"] = meta_title
        
        og_title = result.get("ogTitle", title).strip()
        og_title = og_title.replace('\n', ' ').replace('\r', ' ')
        og_title = re.sub(r'\s+', ' ', og_title)
        result["ogTitle"] = og_title
        
        result.setdefault("metaDescription", result.get("summary", "")[:160].strip())
        result.setdefault("ogDescription", result.get("summary", "")[:160].strip())
        result.setdefault("canonicalUrl", f"https://callrounded.com/blog/{slug}")
        result.setdefault("translationGroup", slug)
        # Si l'IA ne renvoie pas de keywords, on prend ceux du fichier
        if "keywords" not in result or not isinstance(result["keywords"], list):
            result["keywords"] = target_keywords or []
        
        print("‚úÖ SEO optimis√©")
        return result
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur SEO: {e}")
        # Fallback
        slug = article[:50].lower().replace(' ', '-').replace("'", '').replace(",", '').replace("?", '').replace(".", '')
        slug = re.sub(r'[^a-z0-9-]', '', slug)
        title = article.split('\n')[0].replace('#', '').strip()[:60]
        summary = article[:155]
        return {
            "title": title,
            "summary": summary,
            "blog_post": article,
            "slug": slug,
            "readTime": "5 min",
            "tag": "actualites-tendances",
            "keywords": target_keywords or [],
            "metaTitle": title[:60],
            "metaDescription": summary[:160],
            "ogTitle": title,
            "ogDescription": summary[:160],
            "canonicalUrl": f"https://callrounded.com/blog/{slug}",
            "translationGroup": slug
        }


def save_article_for_review(article_data: Dict[str, Any], topic: str, english_data: Dict[str, Any] = None, custom_filename: str = None) -> Path:
    """Sauvegarde l'article dans un fichier pour review au format blog Rounded (FR + EN)."""
    if custom_filename:
        filename = custom_filename
        filepath = ARTICLES_DIR / filename
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        slug = article_data.get("slug", "article")
        filename = f"{timestamp}_{slug}.md"
        filepath = ARTICLES_DIR / filename
    
    # Pr√©parer les contenus FR
    title_fr = article_data.get("title", "Article")
    slug_fr = article_data.get("slug", "article")
    tag_fr = article_data.get("tag", "guides-pratiques")
    read_time = article_data.get("readTime", "N/A")
    focus_kw = article_data.get("focusKeyword", "N/A")
    summary_fr = article_data.get("summary", "")
    keywords = article_data.get("keywords", [])
    html_fr = article_data.get("blog_post", article_data.get("original_content", ""))
    markdown_fr = article_data.get("original_content", "")

    # Date de publication souhait√©e : veille (J-1)
    published_date = datetime.now() - timedelta(days=1)

    # En-t√™te au format "blog Rounded" (comme l'exemple 20251211_135947...)
    content = f"""# {title_fr}

**Sujet original:** {topic}  
**Slug:** {slug_fr}  
**Cat√©gorie:** {tag_fr}  
**Temps de lecture:** {read_time}  
**Focus Keyword:** {focus_kw}  
**G√©n√©r√© le:** {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}

---

## R√©sum√© SEO

{summary_fr}

---

## Mots-cl√©s

{', '.join(keywords)}

---

## Contenu HTML (pour Sanity)

{html_fr}

---

## Contenu Markdown (version originale)

{markdown_fr}
"""

    # Ajouter une section EN apr√®s pour r√©f√©rence (optionnelle)
    if english_data:
        title_en = english_data.get("title", "")
        slug_en = english_data.get("slug", "")
        summary_en = english_data.get("summary", "")
        html_en = english_data.get("blog_post", english_data.get("original_content", ""))
        markdown_en = english_data.get("original_content", html_en)

        content += f"""
---

## Version ANGLAISE

### Title
{title_en}

### Slug
{slug_en}

### R√©sum√© SEO (EN)

{summary_en}

### Contenu HTML EN (pour Sanity)

{html_en}

### Contenu Markdown EN (version originale)

{markdown_en}
"""
    
    filepath.write_text(content, encoding='utf-8')
    return filepath


def display_summary(article_data: Dict[str, Any], filepath: Path):
    """Affiche un r√©sum√© de l'article"""
    print("\n" + "=" * 70)
    print("üìã ARTICLE G√âN√âR√â - √Ä VALIDER")
    print("=" * 70)
    print()
    print(f"üìå Titre: {article_data.get('title', 'N/A')}")
    print(f"üîó Slug: {article_data.get('slug', 'N/A')}")
    print(f"üìã Cat√©gorie: {article_data.get('tag', 'N/A')}")
    print(f"‚è±Ô∏è  Temps de lecture: {article_data.get('readTime', 'N/A')}")
    print(f"üîë Focus Keyword: {article_data.get('focusKeyword', 'N/A')}")
    print()
    print(f"üìù R√©sum√© SEO:")
    print(f"   {article_data.get('summary', 'N/A')}")
    print()
    print("‚îÄ" * 70)
    print("PREVIEW DU CONTENU (premiers 1000 caract√®res):")
    print("‚îÄ" * 70)
    content = article_data.get('blog_post', '')
    # Nettoyer HTML pour preview
    preview = content.replace('<p>', '').replace('</p>', '\n\n')
    preview = preview.replace('<h2>', '\n## ').replace('</h2>', '\n\n')
    preview = preview.replace('<h3>', '\n### ').replace('</h3>', '\n\n')
    preview = preview.replace('<ul>', '').replace('</ul>', '')
    preview = preview.replace('<li>', '‚Ä¢ ').replace('</li>', '\n')
    preview = preview.replace('<strong>', '**').replace('</strong>', '**')
    preview = re.sub(r'<[^>]+>', '', preview)  # Enlever les autres balises HTML
    preview = preview[:1000] + "..." if len(preview) > 1000 else preview
    print(preview)
    print("‚îÄ" * 70)
    print()
    print(f"üíæ Article sauvegard√© dans : {filepath}")
    print(f"   Ouvrez ce fichier pour lire l'article complet")
    print()
    print("‚îÄ" * 70)
    print()


def ask_validation() -> bool:
    """Demande validation √† l'utilisateur"""
    print("‚ùì Voulez-vous publier cet article en PRODUCTION ?")
    print("   - Tapez 'o' pour publier")
    print("   - Tapez 'n' pour annuler")
    print()
    
    try:
        response = input("   Publier ? (o/n) [n]: ").strip().lower()
        return response == 'o'
    except (EOFError, KeyboardInterrupt):
        print("\n‚ö†Ô∏è  Annul√©")
        return False


def fetch_sanity_references(category_slug: str) -> Dict[str, str]:
    """R√©cup√®re les r√©f√©rences Sanity"""
    if not SANITY_TOKEN:
        return {"category": None, "author": None}
    
    url = f"{SANITY_API_URL}/data/query/{SANITY_DATASET}"
    headers = {
        "Authorization": f"Bearer {SANITY_TOKEN}",
        "Content-Type": "application/json"
    }
    
    groq_query = """{
      "category": *[_type == "category" && slug.current == $categorySlug][0]._id,
      "author": *[_type == "author" && name == "Matthieu HUBERT"][0]._id
    }"""
    
    payload = {
        "query": groq_query,
        "params": {"categorySlug": category_slug}
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        if response.status_code == 200:
            result = response.json()
            return result.get("result", {})
    except:
        pass
    
    return {"category": None, "author": None}


def add_article_to_knowledge_base(title: str, slug: str, date: str = None):
    """Ajoute un article √† la base de connaissances pour √©viter les doublons"""
    json_path = Path("data/articles_existants.json")
    
    try:
        if json_path.exists():
            with open(json_path, 'r', encoding='utf-8') as f:
                articles = json.load(f)
        else:
            articles = []
        
        # V√©rifier si l'article existe d√©j√†
        existing = any(art.get("titre") == title or art.get("slug") == slug for art in articles)
        if not existing:
            articles.append({
                "date": date or datetime.now().strftime("%Y-%m-%d"),
                "auteur": "Matthieu HUBERT",
                "titre": title,
                "slug": slug,
                "description": ""
            })
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ Article ajout√© √† la base de connaissances")
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors de l'ajout √† la base: {e}")


def extract_title_from_markdown(content: str) -> str:
    """Extrait le titre depuis le contenu markdown (premi√®re ligne avec #)"""
    if not content:
        return ""
    
    lines = content.split('\n')
    for line in lines:
        line = line.strip()
        if line.startswith('# '):
            # Titre principal (H1)
            title = line[2:].strip()  # Enlever "# "
            return title
        elif line.startswith('## '):
            # Titre secondaire (H2) - utiliser si pas de H1
            if not any(l.strip().startswith('# ') for l in lines[:lines.index(line)]):
                title = line[3:].strip()  # Enlever "## "
                return title
    
    # Fallback : premi√®re ligne non vide
    for line in lines:
        if line.strip():
            return line.strip()
    
    return ""


def generate_english_version(article_data: Dict[str, Any]) -> Dict[str, Any]:
    """G√©n√®re une version anglaise de l'article"""
    if not openai_client:
        return None
    
    print("üåê G√©n√©ration de la version anglaise...")
    
    original_content = article_data.get("original_content", article_data.get("blog_post", ""))
    
    system_prompt = """You are a professional translator specializing in medical and healthcare technology content.

Translate the French article to English while:
- Maintaining the same structure and style
- Keeping the same tone (professional but accessible)
- Preserving all technical terms appropriately
- Keeping mentions of "Donna" and links to https://callrounded.com/cas-usage/secretariat-medical
- Maintaining the same formatting (headings, lists, paragraphs)
- Keeping the same length and depth

Return the translated article in the same format (Markdown with headings, paragraphs, lists)."""
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Translate this French article to English:\n\n{original_content}"}
            ],
            temperature=0.7,
            max_tokens=4000
        )
        
        # Tracker les tokens
        if hasattr(response, 'usage') and response.usage:
            try:
                from utils.token_tracker import track_openai_usage
                track_openai_usage(
                    operation="translate_article",
                    model="gpt-4o-mini",
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    article_title=article_data.get("title", "")
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur tracking tokens: {e}")
        
        english_content = response.choices[0].message.content
        
        # Extraire le titre depuis le contenu markdown si l'IA ne le fournit pas correctement
        extracted_title = extract_title_from_markdown(english_content)
        
        # G√©n√©rer les m√©tadonn√©es SEO en anglais
        seo_prompt = """You are an expert SEO copywriter. Based on the English article, return JSON with:
- title: SEO-optimized title in English (max 65 chars)
- summary: Meta description in English (155-160 chars)
- slug: URL-friendly slug in English (lowercase, hyphens)
- metaTitle: SEO title (50-60 chars)
- metaDescription: SEO meta description (155-160 chars)
- ogTitle: Open Graph title
- ogDescription: Open Graph description (155-160 chars)
- canonicalUrl: Full canonical URL (https://callrounded.com/blog/{slug}-en)"""
        
        seo_response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": seo_prompt},
                {"role": "user", "content": english_content}
            ],
            response_format={"type": "json_object"},
            temperature=0.7
        )
        
        # Tracker les tokens
        if hasattr(seo_response, 'usage') and seo_response.usage:
            try:
                from utils.token_tracker import track_openai_usage
                track_openai_usage(
                    operation="optimize_seo",
                    model="gpt-4o-mini",
                    usage={
                        "prompt_tokens": seo_response.usage.prompt_tokens,
                        "completion_tokens": seo_response.usage.completion_tokens,
                        "total_tokens": seo_response.usage.total_tokens
                    },
                    article_title=article_data.get("title", "")
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur tracking tokens: {e}")
        
        english_seo = json.loads(seo_response.choices[0].message.content)
        
        # Nettoyer le titre pour enlever les caract√®res probl√©matiques
        # Utiliser le titre extrait du markdown si le titre SEO est vide ou probl√©matique
        title = english_seo.get("title", extracted_title).strip()
        if not title or len(title) < 5:
            title = extracted_title
        
        # Garder les caract√®res normaux mais s'assurer qu'il n'y a pas de probl√®mes d'encodage
        title = title.replace('\n', ' ').replace('\r', ' ')
        # Nettoyer les espaces multiples
        title = re.sub(r'\s+', ' ', title)
        # S'assurer que le titre ne d√©passe pas 100 caract√®res (limite raisonnable)
        if len(title) > 100:
            title = title[:97] + "..."
        
        meta_title = english_seo.get("metaTitle", title).strip()[:60]
        meta_title = meta_title.replace('\n', ' ').replace('\r', ' ')
        meta_title = re.sub(r'\s+', ' ', meta_title)
        
        og_title = english_seo.get("ogTitle", title).strip()
        og_title = og_title.replace('\n', ' ').replace('\r', ' ')
        og_title = re.sub(r'\s+', ' ', og_title)
        
        return {
            "original_content": english_content,
            "blog_post": english_content,  # Plain text pour Sanity
            "title": title,
            "summary": english_seo.get("summary", "").strip(),
            "slug": english_seo.get("slug", article_data.get("slug", "") + "-en"),
            "metaTitle": meta_title,
            "metaDescription": english_seo.get("metaDescription", "")[:160].strip(),
            "ogTitle": og_title,
            "ogDescription": english_seo.get("ogDescription", "")[:160].strip(),
            "canonicalUrl": english_seo.get("canonicalUrl", f"https://callrounded.com/blog/{english_seo.get('slug', '')}"),
            "translationGroup": article_data.get("translationGroup", ""),  # M√™me Translation Group
            "language": "en"
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur g√©n√©ration version anglaise: {e}")
        return None


def convert_html_to_plain_text(html_content: str) -> str:
    """Convertit le HTML en texte brut pour l'√©diteur Sanity"""
    # Nettoyer le HTML et convertir en texte simple
    text = html_content
    
    # Remplacer les balises par des sauts de ligne ou rien
    text = re.sub(r'<h2[^>]*>', '\n\n', text)  # H2 ‚Üí double saut
    text = re.sub(r'</h2>', '\n', text)
    text = re.sub(r'<h3[^>]*>', '\n\n', text)  # H3 ‚Üí double saut
    text = re.sub(r'</h3>', '\n', text)
    text = re.sub(r'<p[^>]*>', '\n', text)     # P ‚Üí saut
    text = re.sub(r'</p>', '\n', text)
    text = re.sub(r'<ul[^>]*>', '\n', text)    # UL ‚Üí saut
    text = re.sub(r'</ul>', '\n', text)
    text = re.sub(r'<li[^>]*>', '\n‚Ä¢ ', text)  # LI ‚Üí puce
    text = re.sub(r'</li>', '', text)
    text = re.sub(r'<strong[^>]*>', '**', text)  # Strong ‚Üí markdown bold
    text = re.sub(r'</strong>', '**', text)
    
    # Enlever toutes les autres balises HTML
    text = re.sub(r'<[^>]+>', '', text)
    
    # Nettoyer les espaces multiples et sauts de ligne
    text = re.sub(r'\n{3,}', '\n\n', text)  # Max 2 sauts cons√©cutifs
    text = re.sub(r'[ \t]+', ' ', text)     # Espaces multiples ‚Üí 1 espace
    text = text.strip()
    
    return text


def publish_to_production(article_data: Dict[str, Any], references: Dict[str, str], language: str = "fr") -> bool:
    """Publie directement en PRODUCTION avec tous les champs Sanity"""
    if not SANITY_TOKEN:
        print("‚ùå SANITY_TOKEN manquant dans .env")
        return False
    
    print("\nüöÄ Publication en PRODUCTION...")
    
    slug = article_data.get("slug", "")
    base_id = slug.replace("-", "_") if slug else str(uuid.uuid4())[:8]
    document_id = base_id  # SANS pr√©fixe drafts. = PRODUCTION
    
    # Convertir le contenu en format Block Content Sanity
    # On privil√©gie le HTML optimis√© renvoy√© par l'√©tape SEO,
    # puis on le convertit en blocks Sanity avec html_to_sanity_blocks
    content = article_data.get("blog_post", "")
    body_blocks = None

    if content and "<" in content and ">" in content:
        try:
            body_blocks = html_to_sanity_blocks(content)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur conversion HTML -> Block Content (html_to_sanity_blocks): {e}")
    
    if body_blocks is None:
        # Fallback : utiliser le contenu original en texte (Markdown-like)
        body_text = article_data.get("original_content", content)
        try:
            body_blocks = convert_text_to_sanity_blocks(body_text)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur conversion Block Content: {e}")
            # Fallback: bloc simple
            body_blocks = [{
                "_key": generate_key(),
                "_type": "block",
                "style": "normal",
                "children": [{"_key": generate_key(), "_type": "span", "text": body_text[:500], "marks": []}],
                "markDefs": []
            }]
    
    # Construire le document Sanity complet
    # Date de publication dans Sanity : veille (J-1)
    published_date = datetime.now() - timedelta(days=1)
    
    # Nettoyer le titre pour √©viter les probl√®mes avec les caract√®res sp√©ciaux
    title = article_data.get("title", "").strip()
    title = title.replace('\n', ' ').replace('\r', ' ')
    title = re.sub(r'\s+', ' ', title)
    
    meta_title = article_data.get("metaTitle", title)[:60].strip()
    meta_title = meta_title.replace('\n', ' ').replace('\r', ' ')
    meta_title = re.sub(r'\s+', ' ', meta_title)
    
    post_data = {
        "_id": document_id,
        "_type": "post",
        "title": title,
        "slug": {
            "_type": "slug",
            "current": slug
        },
        "excerpt": article_data.get("summary", "").strip(),
        "body": body_blocks,  # Format Block Content Sanity
        "publishedAt": published_date.isoformat(),
        
        # M√©tadonn√©es SEO
        "metaTitle": meta_title,
        "metaDescription": article_data.get("metaDescription", article_data.get("summary", ""))[:160].strip(),
        
        # Canonical URL
        "canonicalUrl": article_data.get("canonicalUrl", f"https://callrounded.com/blog/{slug}"),
        
        # Translation Group
        "translationGroup": article_data.get("translationGroup", slug),
        
        # Language
        "language": language
    }
    
    # Open Graph et Robots - comment√©s car pas dans le sch√©ma
    # Si votre sch√©ma les accepte, d√©commentez :
    # "ogTitle": article_data.get("ogTitle", article_data.get("title", "")),
    # "ogDescription": article_data.get("ogDescription", article_data.get("summary", ""))[:160],
    # "ogType": "article",
    # "robots": {
    #     "noindex": False,
    #     "nofollow": False
    # }
    
    # Ajouter r√©f√©rences
    if references.get("author"):
        post_data["author"] = {
            "_type": "reference",
            "_ref": references["author"]
        }
    if references.get("category"):
        post_data["categories"] = [{
            "_key": generate_key(),
            "_type": "reference",
            "_ref": references["category"]
        }]
    
    # Main image (si disponible plus tard)
    # post_data["mainImage"] = {
    #     "_type": "image",
    #     "asset": {
    #         "_type": "reference",
    #         "_ref": image_asset_id
    #     },
    #     "alt": "Texte alternatif de l'image",
    #     "caption": "L√©gende de l'image"
    # }
    
    mutation = {
        "mutations": [{
            "create": post_data
        }]
    }
    
    url = f"{SANITY_API_URL}/data/mutate/{SANITY_DATASET}"
    headers = {
        "Authorization": f"Bearer {SANITY_TOKEN}",
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(url, headers=headers, json=mutation, timeout=30)
        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ Article publi√© en PRODUCTION ({language.upper()}) !")
            print(f"   ID: {document_id}")
            print(f"   Titre: {article_data.get('title', 'N/A')}")
            print(f"   Slug: {slug}")
            print(f"   Language: {language}")
            print(f"   Translation Group: {post_data.get('translationGroup', 'N/A')}")
            print(f"   Transaction: {result.get('transactionId', 'N/A')}")
            
            # Ajouter √† la base de connaissances
            if language == "fr":
                add_article_to_knowledge_base(
                    article_data.get('title', ''),
                    slug,
                    datetime.now().strftime("%Y-%m-%d")
                )
            
            print()
            print("üîç L'article est maintenant visible dans votre dashboard Sanity Studio !")
            return True
        else:
            print(f"‚ùå Erreur {response.status_code}: {response.text}")
            return False
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Workflow complet"""
    print("=" * 70)
    print("ü§ñ G√âN√âRATION D'ARTICLE AVEC V√âRIFICATION")
    print("=" * 70)
    print()
    
    # R√©cup√©rer le sujet et la variante √©ventuelle
    variant_arg = None
    topic = None
    
    if len(sys.argv) > 1:
        # Chercher --variant dans les arguments
        args = sys.argv[1:]
        if "--variant" in args:
            idx = args.index("--variant")
            if idx + 1 < len(args):
                variant_arg = args[idx + 1]
                args = args[:idx] + args[idx+2:]
            else:
                args = args[:idx]
        topic = " ".join(args) if args else None
    else:
        try:
            topic = input("üìù Entrez le sujet de l'article: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n‚ùå Annul√©")
            sys.exit(1)
    
    if not topic:
        print("‚ùå Un sujet est requis")
        sys.exit(1)

    # Charger les mots-cl√©s cibles (si disponibles)
    target_keywords = load_target_keywords()
    if target_keywords:
        print("‚úÖ Mots-cl√©s cibles charg√©s depuis data/keywords.json :")
        print("   " + ", ".join(target_keywords))
    else:
        print("‚ÑπÔ∏è Aucun mot-cl√© cible trouv√© dans data/keywords.json (ou fichier vide) ‚Äî l'IA choisira elle-m√™me les keywords SEO.")
    
    # Charger les articles existants
    existing_articles = load_existing_articles()
    if existing_articles:
        print(f"‚úÖ {len(existing_articles)} articles existants charg√©s depuis data/articles_existants.json")
    
    try:
        # 1. V√©rifier les sujets existants (pour information / alerte doublons)
        print("\nüìã √âtape 1/9: V√©rification des sujets existants...")
        existing_topics = get_existing_blog_topics()
        if check_topic_exists(topic, existing_topics):
            print(f"\n‚ö†Ô∏è  ATTENTION: Un article similaire existe d√©j√† sur le blog.")
            print("   On va tout de m√™me proposer de nouvelles id√©es de sujets/angles.\n")
        
        # 2. Recherche web globale sur le sujet
        print("\nüîç √âtape 2/9: Recherche web (Perplexity)...")
        search_query = f"Recherche des donn√©es r√©centes, √©tudes de cas, statistiques 2025 sur {topic}, agents vocaux IA, secr√©tariat m√©dical, cabinets m√©dicaux, automatisation t√©l√©phonique"
        web_results = search_web(search_query)
        
        # 3. G√©n√©ration de 3 variantes de sujets (titre + angle + mini-plan)
        print("\nüìù √âtape 3/9: G√©n√©ration de 3 variantes de sujets (titre + mini-plan)...")
        topic_variants = generate_topic_variants(topic, existing_articles, target_keywords)
        
        print("\n" + "=" * 70)
        print("üìã PROPOSITIONS DE SUJETS - CHOISIS LA VARIANTE")
        print("=" * 70)
        print()
        for idx, v in enumerate(topic_variants, 1):
            print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            print(f"üìÑ VARIANTE {idx}")
            print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            print(f"üìå Titre :", v.get("title", "N/A"))
            print(f"üéØ Angle :", v.get("angle", "N/A"))
            outline = v.get("outline") or []
            if outline:
                print("üß± Mini-plan :")
                for p in outline:
                    print(f"   - {p}")
            print()
        
        print("=" * 70)
        print()
        
        # 4. Demander de choisir la variante de sujet
        if variant_arg:
            try:
                chosen_num = int(variant_arg)
                if chosen_num not in [1, 2, 3]:
                    print(f"‚ö†Ô∏è  Variante {variant_arg} invalide, utilisation de la variante 1 par d√©faut")
                    chosen_num = 1
            except ValueError:
                print(f"‚ö†Ô∏è  Variante {variant_arg} invalide, utilisation de la variante 1 par d√©faut")
                chosen_num = 1
        else:
            try:
                choice = input("üëâ Quelle variante de SUJET choisis-tu ? (1, 2 ou 3) [1]: ").strip()
                if not choice:
                    choice = "1"
                chosen_num = int(choice)
                if chosen_num not in [1, 2, 3]:
                    print("‚ö†Ô∏è  Choix invalide, utilisation de la variante 1 par d√©faut")
                    chosen_num = 1
            except (ValueError, EOFError, KeyboardInterrupt):
                print("‚ö†Ô∏è  Choix invalide ou annul√©, utilisation de la variante 1 par d√©faut")
                chosen_num = 1
        
        chosen_variant = topic_variants[chosen_num - 1]
        print(f"\n‚úÖ Variante de sujet {chosen_num} s√©lectionn√©e : {chosen_variant.get('title', 'N/A')}\n")
        
        # 5. G√©n√©rer l'article complet pour la variante choisie
        print("üìù √âtape 4/9: G√©n√©ration de l'article complet...")
        raw_article = generate_article(chosen_variant, web_results, target_keywords)
        styled = apply_style_refinement(raw_article)
        print("‚úÖ Article g√©n√©r√© et stylis√©\n")

        # 6. SEO pour l'article choisi
        print("üîç √âtape 5/9: Optimisation SEO...")
        article_data = optimize_seo(styled, target_keywords)
        article_data["original_content"] = styled
        print("‚úÖ SEO optimis√©\n")

        # 7. G√©n√©rer version anglaise pour l'article choisi
        print("üåê √âtape 6/9: G√©n√©ration de la version anglaise...")
        english_data = generate_english_version(article_data)
        if english_data:
            print("‚úÖ Version anglaise g√©n√©r√©e\n")
        else:
            print("‚ö†Ô∏è  Version anglaise non g√©n√©r√©e (sera cr√©√©e lors de la publication)\n")
        
        # 8. Cr√©er le fichier de review (FR + EN)
        print("üíæ √âtape 7/9: Cr√©ation du fichier de review (FR + EN)...")
        final_filepath = save_article_for_review(article_data, chosen_variant.get("title", topic), english_data)
        print(f"‚úÖ Fichier de review cr√©√©: {final_filepath.name}\n")
        
        # 9. Afficher r√©sum√© et demander validation
        print("üëÄ √âtape 8/9: Review...")
        display_summary(article_data, final_filepath)
        
        validated = ask_validation()
        
        if validated:
            # 9. R√©cup√©rer r√©f√©rences
            print("\nüîó √âtape 9/9: R√©cup√©ration des r√©f√©rences et publication...")
            category_slug = article_data.get("tag", "actualites-tendances")
            references = fetch_sanity_references(category_slug)
            print("‚úÖ R√©f√©rences r√©cup√©r√©es\n")
            
            # 10. Publication version FR en production
            print("üìù Publication version FRAN√áAISE...")
            success_fr = publish_to_production(article_data, references, language="fr")
            
            if success_fr:
                # 11. Publier version anglaise si g√©n√©r√©e
                if english_data:
                    print("\nüåê Publication version ANGLAISE...")
                    success_en = publish_to_production(english_data, references, language="en")
                    
                    if success_en:
                        print()
                        print("=" * 70)
                        print("‚úÖ TERMIN√â - Articles publi√©s en production (FR + EN) !")
                        print("=" * 70)
                        print(f"\nüíæ Le fichier de review reste disponible: {final_filepath}")
                        print(f"üîó Translation Group: {article_data.get('translationGroup', 'N/A')}")
                    else:
                        print()
                        print("=" * 70)
                        print("‚ö†Ô∏è  Version FR publi√©e, mais erreur sur version EN")
                        print("=" * 70)
                        print(f"\nüíæ Le fichier de review reste disponible: {final_filepath}")
                else:
                    # R√©g√©n√©rer la version anglaise si elle n'a pas √©t√© g√©n√©r√©e avant
                    print("\nüåê G√©n√©ration et publication version ANGLAISE...")
                    english_data = generate_english_version(article_data)
                    
                    if english_data:
                        success_en = publish_to_production(english_data, references, language="en")
                        if success_en:
                            print()
                            print("=" * 70)
                            print("‚úÖ TERMIN√â - Articles publi√©s en production (FR + EN) !")
                            print("=" * 70)
                        else:
                            print()
                            print("=" * 70)
                            print("‚ö†Ô∏è  Version FR publi√©e, mais erreur sur version EN")
                            print("=" * 70)
                    else:
                        print()
                        print("=" * 70)
                        print("‚ö†Ô∏è  Version FR publi√©e, mais g√©n√©ration EN √©chou√©e")
                        print("=" * 70)
                    print(f"\nüíæ Le fichier de review reste disponible: {final_filepath}")
            else:
                print()
                print("=" * 70)
                print("‚ùå ERREUR lors de la publication FR")
                print("=" * 70)
                print(f"\nüíæ Le fichier de review est disponible: {final_filepath}")
                sys.exit(1)
        else:
            print()
            print("=" * 70)
            print("‚ö†Ô∏è  Publication annul√©e")
            print("=" * 70)
            print(f"\nüíæ L'article reste sauvegard√© dans: {final_filepath}")
            print("   Les 3 variantes sont disponibles dans le dossier articles/")
            print("   Vous pouvez le relancer plus tard si vous le souhaitez")
            sys.exit(0)
            
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

